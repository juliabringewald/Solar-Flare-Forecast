{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f2go8SrHbCbm"
      },
      "outputs": [],
      "source": [
        "#LOAD FILE\n",
        "from google.colab import files\n",
        "\n",
        "uploaded_files = files.upload()\n",
        "uploaded_file_name = list(uploaded_files.keys())[0]\n",
        "# Define column names to use after skipping rows --> solution to file loading issue\n",
        "column_names = ['TOTUSJH', 'TOTBSQ', 'TOTPOT', 'TOTUSJZ',\n",
        "    'ABSNJZH', 'SAVNCPP', 'USFLUX', 'AREA_ACR', 'TOTFZ', 'MEANPOT',\n",
        "    'R_VALUE', 'EPSZ', 'SHRGT45'\n",
        "]\n",
        "\n",
        "df = pd.read_csv(uploaded_file_name, delimiter=';', encoding='latin1', skiprows=2, usecols=range(4, 17), names=column_names)\n",
        "\n",
        "print(\"Raw Data:\")\n",
        "print(df.head())\n",
        "\n",
        "df = df.replace({',': '.'}, regex=True)\n",
        "\n",
        "#verify all is converted to float\n",
        "for col in column_names:\n",
        "    df[col] = df[col].astype(float)\n",
        "\n",
        "print(\"\\nDataFrame after conversion to float:\")\n",
        "print(df.head())\n",
        "\n",
        "#handle missing values in df\n",
        "df.fillna(0, inplace=True)\n",
        "\n",
        "#drop target variable\n",
        "df=df.drop(columns=[\"Flare Class\"])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SCALE DATA (feature variables)\n",
        "\n",
        "from itertools import combinations\n",
        "from sklearn.preprocessing import Standardscaler\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit and transform the data\n",
        "scaled_data = scaler.fit_transform(df[column_names])\n",
        "scaled_df = pd.DataFrame(scaled_data, columns=column_names)\n",
        "\n",
        "for column in column_names:\n",
        "    df[column] = scaled_df[column]\n",
        "\n",
        "\n",
        "print(\"\\nScaled Data:\")\n",
        "print(df.head())\n"
      ],
      "metadata": {
        "id": "236mLm_KbFbI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CREATE INTERACTION FEATURES\n",
        "\n",
        "import itertools\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def add_interactions(df):\n",
        "    # Ensure the column names are strings\n",
        "    df.columns = [str(col) for col in df.columns]\n",
        "\n",
        "    # Generate combinations of column names\n",
        "    combos = list(combinations(df.columns, 2))\n",
        "\n",
        "    # Create the column names for the interaction terms\n",
        "    colnames = list(df.columns) + ['_'.join(combo) for combo in combos]\n",
        "\n",
        "    # Create PolynomialFeatures object\n",
        "    poly = PolynomialFeatures(interaction_only=True, include_bias=False)\n",
        "    df_transformed = poly.fit_transform(df)\n",
        "\n",
        "    # Create a DataFrame with the original and interaction columns\n",
        "    df_interactions = pd.DataFrame(df_transformed, columns=colnames)\n",
        "\n",
        "    # Remove columns where all values are zero (no interaction)\n",
        "    noint_indices = [i for i, x in enumerate((df_interactions == 0).all()) if x]\n",
        "    df_interactions = df_interactions.drop(df_interactions.columns[noint_indices], axis=1)\n",
        "\n",
        "    return df_interactions\n",
        "\n",
        "# Add interaction features\n",
        "df_interactions = add_interactions(df)\n",
        "print(\"Data with Interaction Features:\")\n",
        "print(df_interactions.head())\n",
        "\n"
      ],
      "metadata": {
        "id": "msXfWYgfboDM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# COVARIANCE HEATMAP\n",
        "\n",
        "cov_matrix = df.cov()\n",
        "\n",
        "# Plot the covariance matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "sns.heatmap(cov_matrix, annot=True, cmap='coolwarm')\n",
        "plt.title(\"Covariance Matrix Heatmap\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ERt_QyBgciPd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# DETERMINE CUMMULATIVE EXPLAINED VARIANCE (= total amount of variance captured by a certain number of principal components)\n",
        "\n",
        "# Ensure your data (X) is standardized before applying PCA\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA()\n",
        "# Fit PCA on the data\n",
        "pca.fit(df_interactions)\n",
        "\n",
        "# Plot the cumulative explained variance\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(np.cumsum(pca.explained_variance_ratio_), marker='o', linestyle='--')\n",
        "plt.xlabel('Number of Principal Components')\n",
        "plt.ylabel('Cumulative Explained Variance')\n",
        "plt.title('Cumulative Explained Variance by Principal Components')\n",
        "plt.grid()\n",
        "plt.show()\n",
        "\n",
        "# Determine the number of components to explain desired variance (e.g., 95%)\n",
        "variance_threshold = 0.95\n",
        "n_components = np.argmax(np.cumsum(pca.explained_variance_ratio_) >= variance_threshold) + 1\n",
        "\n",
        "print(f\"Number of components to explain {variance_threshold*100}% variance: {n_components}\")\n"
      ],
      "metadata": {
        "id": "OX9uo6F5cl5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# TARGET VARAIABLE (Solar flare class) ENCODING FROM GOES CLASS TO NUMERIC (hierarchical)\n",
        "\n",
        "def encode_flare_class(flare_class):\n",
        "    main_class = flare_class[0]\n",
        "    sub_class = int(flare_class[1:])\n",
        "    mapping = {'A': 0, 'B': 100, 'C': 200, 'M': 300, 'X': 400}\n",
        "    return mapping[main_class] + sub_class\n",
        "\n",
        "df['flare_class_encoded'] = df['Flare Class'].apply(encode_flare_class)\n",
        "\n"
      ],
      "metadata": {
        "id": "7xf2ntlfc5He"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}